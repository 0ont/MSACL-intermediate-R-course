---
title: 'Lesson 7: Modelling the data'
author: "Adam Zabell"
date: "1/2/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(modelr)
library(magrittr)
library(lubridate)
```

# Data Modelling
After enough measurements have been made to consider your data a dataset, the next step is to explore that dataset to find trends and outliers. Modelling reduces the complexity from individual observations but can only operate within the limits of the model. The linear model shown below doesn't fit as well as the quadratic model, but deciding whether the quadratic model makes physical sense is not something the model can understand.
```{r drawing_two_models}
# data taken from http://www.theanalysisfactor.com/r-tutorial-4/
y <- c(126.6, 101.8, 71.6, 101.6, 68.1, 62.9, 45.5, 41.9, 46.3, 34.1, 38.2, 41.7, 24.7, 41.5, 36.6, 19.6, 22.8, 29.6, 23.5, 15.3, 13.4, 26.8, 9.8, 18.8, 25.9, 19.3)
x <- c(0, 1, 2, 4, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30)
x2 <- x**2
raw <- tibble(x,y,x2)
linearModel <- lm(y ~ x, data=raw)
quadraticModel <- lm(y ~ x + x2, data=raw)
plot(x,y)
sequenceX <- seq(0,30,by=0.1)
modeldata <- data.frame(x=sequenceX,x2=sequenceX**2)
sequenceYL <- predict(linearModel,modeldata)
sequenceYQ <- predict(quadraticModel,modeldata)
lines(sequenceX,sequenceYL,col='blue',lwd=2)
lines(sequenceX,sequenceYQ,col='darkgreen',lwd=2)
```

## Parsing dates and times with *lubridate*
The context for most of the data being discussed in this class involves a time series, where each batch is collected at a specific date and knowing that order of collection is a necessary component of the analysis. Dates within **R** are interpreted using the IEEE "POSIX" format, but reading data from an exterior source such as a CSV means this information arrives as a string. The **lubridate** package helps to convert strings to POSIX, or combine information from multiple columns of a data.frame into a usable date format. 
```{r lubridate, error=TRUE}
stringOfDates <- c("2017-12-15","2017-Dec-16","2017-12-17","2017-12-18","2017-12-19","2017-12-20","2017-12-21")
oneWeek <- ymd(stringOfDates)
stringOfDates + 7 
oneWeek + 7
tableOfDates <- tibble(year=rep(2017,8),
                       month=c(rep(4,3),rep(5,5)),
                       day=seq(1,8),
                       hour=rep(4,8),
                       minute=c(15,29,40,45,0,58,9,11))
tableOfDates %$% make_date(year,month,day) # uses the magrittr 'not a pipe' operator
tableOfDates %$% make_datetime(year,month,day,hour,minute)
```
It helps to remember **R** uses "yyyy-mm-dd" as the default order for dates when viewing or exporting.

## Symbolic formula notation
The difficulty in using **R** for scientific modelling is that the language was built for statistical modelling. This is most obvious in the Wilkinson-Rogers notation for formula design. It is a versitile and powerful way to quickly describe the interactions between variables, but lacks an intuitive way to express basic transformations (quadratic, logrithmic, etc)

Formula Notation  | Common Understanding
---------------- | --------------------
y ~ x | y = *a0* + *a1*x
y ~ x - 1 | y = *a1*x
y ~ x + z | y = *a0* + *a1*x + *a2*z 
y ~ x * z | y = *a0* + *a1*x + *a2*z + *a3*xz
y ~ x + I(x^2) | y = *a0* + *a1*x + *a2*x^2^
y ~ Gauss(amplitude,mu,sigma,x) | fit the data to a formula, defined elsewhere

Wilkinson-Rogers notation comes into it's own when looking for interactions (and their relative importance) between the dependent variable and a collection of independent ones: the formula _y ~ Height * Weight * Age_ would solve for eight coefficients, the linear combination for each of the three terms and the intercept. By comparison, the formula _y ~ Height + Weight + Age_ would solve four coefficients, ignoring any interaction between Height, Weight, and Age.

## Linear modelling 
Most of time a linear model, or transforming the data into a linear model, is sufficient to establish the key relationships between variables. First, expand the *raw* data.frame with additional observations and two more descriptors for the data.
```{r linear_modelling_setup}
newraw <- raw %>% 
          mutate(y = raw$y * rnorm(nrow(raw),mean=1,sd=0.3)) %>% 
          rbind(raw) %>%
          arrange(x)
newraw$instrument <- c(rep("Coarse",12),rep("Fine",40))
newraw$date <- c("07-08-2017","10-08-2017") %>%
               rep(times=26) %>%
               dmy() # as a single line without pipes: ymd( rep(c("2017-08-07","2017-08-10"),times=26) )
```

Now we can explore the simple linear model in context of the instrument or the date of collection.
```{r linear_modelling_run}
fullModel <- lm(y ~ x * date * instrument, data=newraw)
interactingTerms <- lm(y ~ x * instrument, data=newraw)
noninteractingTerms <- lm(y ~ x + instrument, data=newraw)
grid <- newraw %>%
  data_grid(x,date,instrument) %>%
  gather_predictions(interactingTerms,noninteractingTerms)
ggplot(newraw, aes(x,y,shape=factor(date))) +
  geom_point() +
  geom_line(data=grid, aes(y=pred,color=model)) +
  coord_cartesian(ylim=range(y)) +
  facet_wrap(~ instrument)
```

## Nonlinear modelling
Although not common when looking for relationships between variables, nonlinear models are a concern for other aspects of data evaluation (e.g. fitting a chromatogram to a Gaussian curve) or assay validation (e.g. instrument response as a function of concentration). Unlike linear modelling, which will find a single solution starting from the data itself, a nonlinear model must iterate from a starting guess for each variable. A well formed guess should converge on the correct value, but there is no general method for finding that guess. 
```{r nonlinear_modelling, error=TRUE}
Gauss = function(amplitude,mu,sigma,x){ amplitude*exp(-0.5*((x - mu)/sigma)^2) }
x <- seq(1,5,by=0.1)
yExact <- Gauss(10000,3,0.4,x)
yNoisy <- yExact * rnorm(length(yExact),mean=1,sd=0.3)
noisyCurve <- tibble(x=x,y=yNoisy)
perfectGuess <- nls(y ~ Gauss(a,m,s,x), 
                    start=list(a=10000,m=3,s=0.4),
                    data=noisyCurve)
okayGuess <- nls(y ~ Gauss(a,m,s,x), 
                 start=list(a=max(yNoisy),m=x[yNoisy==max(yNoisy)],s=1),
                 data=noisyCurve)
badGuess <- nls(y ~ Gauss(a,m,s,x), 
                start=list(a=1,m=1,s=1),
                data=noisyCurve)
forPlot <- noisyCurve %>% rename(yNoisy=y) %>% cbind(yExact) %>% gather(yType,y,-x)
grid <- forPlot %>%
  data_grid(x=seq_range(x,100),y) %>%
  gather_predictions(perfectGuess,okayGuess)
ggplot(forPlot, aes(x,y)) +
  geom_line(data=grid, aes(y=pred,color=model,linetype=model), size=1) +
  geom_point(aes(shape=yType)) +
  scale_shape_manual(values=c(20,1),name="data source")
summary(perfectGuess)
```

Understanding the reasoning for nonlinear model selection is a critical step before attempting that fit. The risk of non-convergence means the nonlinear model should be avoided, if possible.

# Summary
* **lubridate** functions convert a text string to a date
* statistial modelling with Wilkinson-Rogers notation builds possible interactions into the analysis
* scientific modelling with Wilkinson-Rogers notation requires careful understanding of terms
* linear models are strongly recommended whenever possible, and may require data transformation to achieve
* nonlinear models are dependant on a starting guess