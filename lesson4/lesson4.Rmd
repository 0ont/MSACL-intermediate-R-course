---
title: 'Lesson 4: Data manipulation in the tidyverse'
author: "Patrick Mathias"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(janitor)
library(readxl)
```

## A brief diversion to discuss the tidyverse

According to the official tidyverse [website](https://www.tidyverse.org/), "the tidyverse is an *opinionated* collection of R packages designed for data science." We've gotten a flavor of tidyverse functionality by using the readr packages and will wade deeper into the tidyverse in the next lessons. Because the tidyverse was not a component of the introductory MSACL data science course in previous years, we are going to cover basic functionality of many tidyverse packages throughout the rest of the course. Many of the data manipulation concepts will probably be familiar but the tidyverse offers a consistent interface for functions. Data is consistently the first argument for functions, and that enables compatibility with pipes. The tidyverse includes its own version of a data frame, the [tibble](http://tibble.tidyverse.org/articles/tibble.html), with the primary advantages being nicer printing of output and more predictable behavior with subsetting.

One advantage of using the tidyverse packages is the relatively robust support documentation around these packages. Stack Overflow is often a go to for troubleshooting but many tidyverse packages have nice vignettes and other online resources to help orient you to how the package functions work. There is a freely available online book, [R for Data Science](http://r4ds.had.co.nz/) that covers the tidyverse (and more). [Cheat Sheets](https://www.rstudio.com/resources/cheatsheets/) provided by RStudio also provide great quick references for tidyverse and other packages.

You can load the core tidyverse packages by loading tidyverse: `library(tidyverse)`. [ggplot2](http://ggplot2.tidyverse.org/) is probably the most popular tidyverse package and arguably the go to for sophisticated visualizations in R, but inevitably data will need to be manipulated prior to plotting. So the two workhorse packages for many applications are [dplyr](http://dplyr.tidyverse.org/) and [tidyr](http://tidyr.tidyverse.org/), which we will cover in this lesson.

## Manipulating data with dplyr

The [dplyr package](http://dplyr.tidyverse.org/) provides functions to carve, expand, and collapse a data frame (or tibble).

### Carving your data set

Reducing a data set to a subset of columns and/or rows are common operations, particularly on the path to answering a specific set of questions about a data set. If you need to go from a large number of columns (variables) to a smaller set, `select()` allows you to select specific columns by name. If you need only a subset of rows from your data set, `filter()` allows you to pick rows (cases) based on values, ie. you can subset your data based on logic.

Let's take these for a spin using the data we started examining in the last lesson.

Review the type of data we were working with:
```{r}
samples_jan <- read_csv(
  here("data", "2017-01-06.csv"),
  col_types = cols(
    compoundName = col_factor(NULL),
    sampleType = col_factor(NULL)
    )
  ) %>% 
  clean_names()
str(samples_jan)
```

Let's say we don't need the last two logical columns and want to get rid of them. We can use `select()` and provide a range of adjacent variables:
```{r}
samples_jan %>%
  select(batch_name:expected_concentration)
```

Or we only care about the first 3 variables plus the concentration:
```{r}
samples_jan %>%
  select(batch_name:compound_name, concentration)
```

Now let's carve the data set in the other direction. If we only care about the morphine data, we can use `filter()` to pick those rows based on a logical condition:
```{r}
samples_jan %>%
  filter(compound_name == "morphine")
```

Or maybe we want to examine only the unknown samples with a concentration greater than 0:
```{r}
samples_jan %>%
  filter(sample_type == "unknown", concentration > 0)
```

Note that a comma in the filter state implies a logical AND - condition A and condition B. You could include an OR condition as well using the pipe character | - condition A | condition B.
```{r}
samples_jan %>%
  filter(sample_type == "unknown" | concentration > 0)
```

***Exercise***
Carve the January data set in both directions. We want sample information (batch, sample, compound) and ion ratio data for only oxycodone measurements in unknown sample types. Provide a summary of the data.

```{r}
samples_jan_oxy_ir <- samples_jan %>%
  filter() %>%
  select()
summary()
```

### Expanding your data set

Another common data manipulation task is adding or replacing columns that are derived from data in other columns. The `mutate()` function provides a quick and clean way to add additional variables that can include calculations, evaluating some logic, string manipulation, etc. You provide the function with the following argument(s): name of the new column = value. For example, if we continue with our January sample data set that includes concentrations and expected concentrations for standards, we can calculate the ratio of concentration to expected:
```{r}
samples_jan %>%
  filter(sample_type == "standard", expected_concentration > 0) %>%
  mutate(conc_ratio = concentration/expected_concentration) %>%
  select(batch_name:compound_name, concentration, expected_concentration, conc_ratio)
```

Notice that we got around the issue of dividing by 0 by filtering for expected concentrations above 0. However, you may want to include these yet don't want R to throw an error. How can you deal with edge cases like this? `mutate()` borrows from SQL (Structured Query Language) and offers a `case_when` syntax for dealing with different cases. The [syntax](http://dplyr.tidyverse.org/reference/case_when.html) takes some getting used to but this can be an invaluable tool. Let's do the same calculation but spell out the case when expected_concentration is 0 and add a small number to numerator and denominator in that case:
```{r}
samples_jan %>%
  filter(sample_type == "standard") %>%
  mutate(
    conc_ratio = case_when(
      expected_concentration == 0 ~ (concentration + 0.001)/(expected_concentration + 0.001),
      TRUE ~ concentration/expected_concentration
    )
  ) %>%
  select(batch_name:compound_name, concentration, expected_concentration, conc_ratio)
```

Another common operation manipulation is wrangling dates. The [lubridate package](http://lubridate.tidyverse.org/) offers a helpful toolset to quickly parse dates and times. The bread and butter parsing functions are named intuitively based on the order of year, month, date, and time elements. For example, `mdy("1/20/2018")` will convert the string into a date that R can use. There are other useful functions like `month()` and `wday()` that pull out a single element of the date to use for grouping operations, for example. Let's work with a different January data set that has batch data and parse the collection dates in a variety of ways:
```{r}
batch_jan <- read_excel(here("data", "2017-01-06.xlsx")) %>%
  clean_names()
batch_jan %>%
  mutate(
    collect_month = month(batch_collected_timestamp),
    collect_day_of_week = wday(batch_colllected_timestamp),
    collect_week = week(batch_collected_timestamp),
    collect_week_alt = floor_date(batch_collected_timestamp, unit = "week")
  )
```


### Collapse (summarize) your data set

## Shaping and tidying data with tidyr
